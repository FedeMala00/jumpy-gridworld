{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import numpy as np\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseEnvironment(ABC):\n",
    "    \n",
    "    @abstractmethod\n",
    "    # Override this method to return the set of states\n",
    "    def get_states(self):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    # Override this method to return the set of actions available in the state\n",
    "    def get_actions(self, state):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_all_actions(self):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def transition(self, action):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    # Ovveride this method to implement action execution\n",
    "    def do_action_and_get_reward(self, action):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld(BaseEnvironment):\n",
    "    \n",
    "    UP = (-1,0)\n",
    "    DOWN = (1,0)\n",
    "    LEFT = (0,-1)\n",
    "    RIGHT = (0,1)\n",
    "    \n",
    "    label = {(-1,0):\"UP\", (1,0):\"DOWN\", (0,-1):\"LEFT\", (0,1):\"RIGHT\"}\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.states = list()\n",
    "        for row in range(5):\n",
    "            for col in range(5):\n",
    "                self.states.append((row,col))\n",
    "        self.actions = dict()\n",
    "        self.actions[(0,0)] = [GridWorld.RIGHT, GridWorld.DOWN]\n",
    "        self.actions[(0,1)] = [GridWorld.RIGHT, GridWorld.LEFT, GridWorld.DOWN]\n",
    "        self.actions[(0,2)] = [GridWorld.RIGHT, GridWorld.LEFT, GridWorld.DOWN]\n",
    "        self.actions[(0,3)] = [GridWorld.RIGHT, GridWorld.LEFT, GridWorld.DOWN]\n",
    "        self.actions[(0,4)] = [GridWorld.LEFT, GridWorld.DOWN]\n",
    "        self.actions[(1,0)] = [GridWorld.RIGHT, GridWorld.DOWN, GridWorld.UP]\n",
    "        self.actions[(1,1)] = [GridWorld.RIGHT, GridWorld.LEFT, GridWorld.DOWN, GridWorld.UP]\n",
    "        self.actions[(1,2)] = [GridWorld.RIGHT, GridWorld.LEFT, GridWorld.DOWN, GridWorld.UP]\n",
    "        self.actions[(1,3)] = [GridWorld.RIGHT, GridWorld.LEFT, GridWorld.DOWN, GridWorld.UP]\n",
    "        self.actions[(1,4)] = [GridWorld.LEFT, GridWorld.DOWN, GridWorld.UP]\n",
    "        self.actions[(2,0)] = [GridWorld.RIGHT, GridWorld.DOWN, GridWorld.UP]\n",
    "        self.actions[(2,1)] = [GridWorld.RIGHT, GridWorld.LEFT, GridWorld.DOWN, GridWorld.UP]\n",
    "        self.actions[(2,2)] = [GridWorld.RIGHT, GridWorld.LEFT, GridWorld.DOWN, GridWorld.UP]\n",
    "        self.actions[(2,3)] = [GridWorld.RIGHT, GridWorld.LEFT, GridWorld.DOWN, GridWorld.UP]\n",
    "        self.actions[(2,4)] = [GridWorld.LEFT, GridWorld.DOWN, GridWorld.UP]\n",
    "        self.actions[(3,0)] = [GridWorld.RIGHT, GridWorld.DOWN, GridWorld.UP]\n",
    "        self.actions[(3,1)] = [GridWorld.RIGHT, GridWorld.LEFT, GridWorld.DOWN, GridWorld.UP]\n",
    "        self.actions[(3,2)] = [GridWorld.RIGHT, GridWorld.LEFT, GridWorld.DOWN, GridWorld.UP]\n",
    "        self.actions[(3,3)] = [GridWorld.RIGHT, GridWorld.LEFT, GridWorld.DOWN, GridWorld.UP]\n",
    "        self.actions[(3,4)] = [GridWorld.LEFT, GridWorld.DOWN, GridWorld.UP]\n",
    "        self.actions[(4,0)] = [GridWorld.RIGHT, GridWorld.UP]\n",
    "        self.actions[(4,1)] = [GridWorld.RIGHT, GridWorld.LEFT, GridWorld.UP]\n",
    "        self.actions[(4,2)] = [GridWorld.RIGHT, GridWorld.LEFT, GridWorld.UP]\n",
    "        self.actions[(4,3)] = [GridWorld.RIGHT, GridWorld.LEFT, GridWorld.UP]\n",
    "        self.actions[(4,4)] = [GridWorld.LEFT, GridWorld.UP]\n",
    "        self.all_actions = [GridWorld.RIGHT, GridWorld.LEFT, GridWorld.DOWN, GridWorld.UP]\n",
    "        self.current_state = (0,0)\n",
    "    \n",
    "    def get_states(self):\n",
    "        return self.states\n",
    "        \n",
    "    def get_actions(self, state):\n",
    "        return self.actions[state]\n",
    "    \n",
    "    def get_all_actions(self):\n",
    "        return self.all_actions\n",
    "    \n",
    "    def transition(self, current_state, action):\n",
    "        new_state = (current_state[0] + action[0], current_state[1] + action[1])\n",
    "        # Check if the new state is within the grid\n",
    "        if new_state in self.states:\n",
    "            return new_state\n",
    "        # If the new state is off the grid, return the current state\n",
    "        else:\n",
    "            return current_state\n",
    "    \n",
    "    def do_action_and_get_reward(self, action):\n",
    "        # If the current state is A or B, move to A' or B' respectively and return the corresponding reward\n",
    "        if self.current_state == (0,1):\n",
    "            self.current_state = (4, 1)\n",
    "            return self.current_state, 10\n",
    "        elif self.current_state == (0,3):\n",
    "            self.current_state = (2, 3)\n",
    "            return self.current_state, 5\n",
    "\n",
    "        # Compute the next state based on the action\n",
    "        new_state = self.transition(self.current_state, action)\n",
    "        \n",
    "        # If the action would take the agent off the grid, return reward -1\n",
    "        if new_state == self.current_state:\n",
    "            return self.current_state, -1\n",
    "        \n",
    "        # For all other states, update the current state and return reward 0\n",
    "        else:\n",
    "            self.current_state = new_state\n",
    "            return self.current_state, 0\n",
    "        \n",
    "    def reset(self):\n",
    "        self.current_state = (0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasePolicy(ABC):\n",
    "    \n",
    "    @abstractmethod\n",
    "    # Ovveride this method to implement policy application\n",
    "    # Returns the action given the state\n",
    "    def apply(self, state):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyPolicy(BasePolicy):\n",
    "    \n",
    "    def __init__(self, environment, Q_table, epsilon):\n",
    "        self.environment = environment\n",
    "        self.Q_table = Q_table\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def apply(self, state):\n",
    "        actions = self.environment.get_actions(state)\n",
    "        if random.random() < self.epsilon:\n",
    "            # Choose an action at random with probability epsilon\n",
    "            return random.choice(actions)\n",
    "        else:\n",
    "            # Choose the best action accordin to Q_table with probability 1-epsilon\n",
    "            # If all actions have the same Q-value then break ties randomly\n",
    "            max_action_value = -1 * sys.float_info.max\n",
    "            best_action = random.choice(actions)\n",
    "            for action in actions:\n",
    "                if self.Q_table[state][action] > max_action_value:\n",
    "                    max_action_value = self.Q_table[state][action]\n",
    "                    best_action = action\n",
    "            return best_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSA:\n",
    "    \n",
    "    def __init__(self, environment, gamma, alpha, epsilon, episodes):\n",
    "        self.environment = environment\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.episodes = episodes\n",
    "        self.Q_table = dict()\n",
    "        # Initialize the value of each state-action pair to 0\n",
    "        for state in environment.get_states():\n",
    "            self.Q_table[state] = dict()\n",
    "            for action in environment.get_actions(state):\n",
    "                self.Q_table[state][action] = 0\n",
    "        # Use epsilon-greedy policy for learning\n",
    "        self.policy = EpsilonGreedyPolicy(environment, self.Q_table, epsilon)\n",
    "            \n",
    "    def apply(self):\n",
    "        for e in range(self.episodes):\n",
    "            state = self.environment.current_state \n",
    "            action = self.policy.apply(self.environment.current_state) \n",
    "            for _ in range(20):\n",
    "                next_state, reward = self.environment.do_action_and_get_reward(action)\n",
    "                # next_state = self.environment.current_state\n",
    "                next_action = self.policy.apply(next_state)\n",
    "                temporal_difference = self.gamma * self.Q_table[next_state][next_action] - self.Q_table[state][action]\n",
    "                self.Q_table[state][action] += self.alpha * (reward + temporal_difference)\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "            # Must reset the environment before trying another episode\n",
    "            self.environment.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_world = GridWorld()\n",
    "sarsa = SARSA(grid_world, 0.9, 0.3, 0.05, 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarsa.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyPolicy(BasePolicy):\n",
    "    \n",
    "    def __init__(self, environment, Q_table):\n",
    "        self.environment = environment\n",
    "        self.Q_table = Q_table\n",
    "        \n",
    "    def apply(self, state):\n",
    "        actions = self.environment.get_actions(state)\n",
    "        max_action_value = -1 * sys.float_info.max\n",
    "        best_action = None\n",
    "        for action in actions:\n",
    "            if self.Q_table[state][action] > max_action_value:\n",
    "                max_action_value = self.Q_table[state][action]\n",
    "                best_action = action\n",
    "        return best_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_policy = GreedyPolicy(grid_world, sarsa.Q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RIGHT LEFT LEFT DOWN LEFT \n",
      "RIGHT UP UP UP LEFT \n",
      "UP UP LEFT LEFT UP \n",
      "UP UP LEFT UP LEFT \n",
      "UP UP LEFT UP UP \n",
      "The policies do not match at (1, 3).\n",
      "The number of errors is 1.\n"
     ]
    }
   ],
   "source": [
    "# Print the policy\n",
    "generated_policy = []\n",
    "for ri in range(5):\n",
    "    row = []\n",
    "    for co in range(5):\n",
    "        action = GridWorld.label[greedy_policy.apply((ri,co))]\n",
    "        print(action, end=' ')\n",
    "        row.append(action)\n",
    "    print()\n",
    "    generated_policy.append(row)\n",
    "\n",
    "# Correct policy derived from the Sutton and Barto book\n",
    "correct_policy = [\n",
    "    [['RIGHT'], ['UP', 'RIGHT', 'LEFT', 'DOWN'], ['LEFT'], ['UP', 'RIGHT', 'LEFT', 'DOWN'], ['LEFT']],\n",
    "    [['UP', 'RIGHT'], ['UP'], ['UP', 'LEFT'], ['LEFT'], ['LEFT']],\n",
    "    [['UP', 'RIGHT'], ['UP'], ['UP', 'LEFT'], ['UP', 'LEFT'], ['UP', 'LEFT']],\n",
    "    [['UP', 'RIGHT'], ['UP'], ['UP', 'LEFT'], ['UP', 'LEFT'], ['UP', 'LEFT']],\n",
    "    [['UP', 'RIGHT'], ['UP'], ['UP', 'LEFT'], ['UP', 'LEFT'], ['UP', 'LEFT']]\n",
    "]\n",
    "# Compare the two policies\n",
    "num_errors = 0\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        if generated_policy[i][j] not in correct_policy[i][j]:\n",
    "            num_errors += 1\n",
    "            print(f\"The policies do not match at ({i}, {j}).\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal value function:\n",
      "[22.0, 24.4, 22.0, 19.4, 17.5]\n",
      "[19.8, 22.0, 19.8, 17.8, 16.0]\n",
      "[17.8, 19.8, 17.8, 16.0, 14.4]\n",
      "[16.0, 17.8, 16.0, 14.4, 13.0]\n",
      "[14.4, 16.0, 14.4, 13.0, 11.7]\n",
      "\n",
      "\n",
      "Obtained State-value function:\n",
      "[21.1980094486983, 23.81067852666498, 21.322992434445673, 18.45910190342609, 16.356612384721192]\n",
      "[19.038142694510267, 21.360445741463632, 19.186434204057612, 16.631831948582104, 14.752233321865821]\n",
      "[17.04250594499519, 17.793609062540483, 17.045404513591436, 14.951552870830037, 13.137846308963441]\n",
      "[15.181008074564147, 17.17367444288248, 14.71157826834008, 13.390902475181583, 12.01122145204464]\n",
      "[13.535517584690703, 15.396906108468707, 13.591948029876452, 11.999342010609228, 10.779859013454503]\n"
     ]
    }
   ],
   "source": [
    "state_value_function = [[0]*5 for _ in range(5)]  # Initialize a 5x5 grid with zeros\n",
    "# Optimal value function derived from the Sutton and Barto book\n",
    "optimal_value_function = [\n",
    "    [22.0, 24.4, 22.0, 19.4, 17.5],\n",
    "    [19.8, 22.0, 19.8, 17.8, 16.0],\n",
    "    [17.8, 19.8, 17.8, 16.0, 14.4],\n",
    "    [16.0, 17.8, 16.0, 14.4, 13.0],\n",
    "    [14.4, 16.0, 14.4, 13.0, 11.7]\n",
    "]\n",
    "print(\"Optimal value function:\")\n",
    "for row in optimal_value_function:\n",
    "    print(row)\n",
    "\n",
    "for ri in range(5):\n",
    "    for co in range(5):\n",
    "        state = (ri, co)\n",
    "        actions = sarsa.environment.get_actions(state)\n",
    "        # Find the maximum action-value for the current state\n",
    "        state_value_function[ri][co] = max(sarsa.Q_table[state][action] for action in actions)\n",
    "print(\"\\n\")        \n",
    "print(\"Obtained State-value function:\")\n",
    "for row in state_value_function:\n",
    "    print(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
